---
title: 'Agile (data) science: a (draft) manifesto'
author: "J. J. Merelo"
date: "3/4/2021"
output:
  pdf_document: default
  html_document: default
bibliography: agile.bib
abstract: Science has a data management problem, as well as a project management problem.
  While industry data science teams have embraced the *agile* mindset, and adopted
  or created all kind of tools to create reproducible workflows, academia-based science
  is still (mostly) mired in a mindset that's focused on a single final product (a
  paper), without focusing on incremental improvement and, over all, reproducibility.
  In this report we argue towards the adoption of the agile mindset and agile data
  science tool in academia, to make a more responsible, and over all, reproducible
  science.
---

## Introduction

By *agile*, we usually imply a mindset that is applied to the whole software development lifecycle which is customer-centered and focused on continuous improvement of increasingly complex minimally viable products. The name comes from the Agile Manifesto [@beck2001manifesto], literally "Manifesto for agile software development". This manifesto has certainly changed the way software at large is developed, and become mainstream, spawning many different methodologies and best practice guidelines. It has proved to be an efficient way of carrying out all kind of projects, from small to large-scale ones, mitigating the presence of bugs and proving to be more efficient [@abrahamsson2017agile] than the methodology that prevailed previously (and still today in many sectors), generally called *waterfall* [@andrei2019study], which separated (or *siloed*) different teams doing from the specification to the testing, with every team acting at different parts of the lifecycle.

Despite being prevalent in software development (and, in general, project development) environments, it certainly has not reached science at large, which arguably follows a method that closely follows the waterfall methodology.

Since data science and engineering has become an integral part of the workflow in many companies, *agile* data science is, mostly, the way it's done. Again, this is mostly because data science is mostly done in the industry, and not in academia, which does not have the same kind of workflows to deal with its own data.

Our intention is to try and put science back in data science. We will try and examine critically how science is done, what are the main reasons why this agile mindset is not being used in science, how would *agile* concepts translate to science, and eventually what agile data science and science at large woudl look like.

We will first present what attempts have been made to translate agile concepts to the (academic) world of science.

## State of the art

Despite being an age-old pursuit and probably, as such, ripe for disruption, science has been done in pretty much the same way for years. In very rough brushstrokes, it starts with application for funding, that includes a workplan, that generally works hierarchically from principal investigators to senior and then junior researchers, producing a series of artifacts which always include papers (which are snapshots of the state of the art), and in some cases software, protocols, or even, in some limited areas (mainly astronomy and medicine), workflows.

This situation has been challenged repeatedly, lately, mainly after the introduction of the aforementioned agile manifesto. In [@agile-science], which is essentially a presentation and not a formal paper, several proposal are made to apply agile "methods" in research; something that has been proposed repeatedly in later years, for instance in this blog post [@carattino-agile-dev] and even in this paper [@9140255] which specifies an agile methodology, Scrum, and how it can be applied specifically to data science projects. As a matter of fact, there were several attempts to raise the issue again and bring it to the attention of the research community: a blog post introduced *agile research* [@AgileResearchBlog] and even drafted an Agile Research Manifesto [@AgileResearch]. This was almost totally forgotten until it was brought up two years ago in a blog called "Agile Science" [@XavierAgileScience]. Independently, some researchers proposed an (almost) ultimatum for Agile Research in [@way2009agile], and eventually it became fruitful in a restricted environment, mHealth, in  [@wilson2018agile]. This goes to show that it's still part of the fringe, and has not been incorporated either to funding agencies guidelines, or to the common science and research practice.

This is certainly related with Open Science: Open Science adapts the main ideas of open source software development to the publication of scientific results and artifacts; the push for Open Science [@robson_opensci] has provided with new venues and new ways of understanding and producing science. However, the uptake of new methodologies is still very slow. While most companies have created pipelines for data management [@uberlyftothers], there are neither clear guidelines or best practices nor resources where scientific data management can be done at scale and, what's more important, in a way that can have a (positive) outcome for your career.

## Hypothesis on agile science

We certainly need to acknowledge first that science, the way it is now, has many problems. Many of them start and end with funding, but we should also realize that using XIX century formats to publish XXI century research leaves a lot to be desired. However, the fundamental problem is not in products, is in practitioner's workflows themselves, and this ends in frustration and, when major crisis like COVID-19 strike, major problems carrying out much needed research that can be used as a foundation for the next, also necessary, step. So we will try to present, and defend, a series of hypothesis that would be the foundation of agile science, and that would contribute to solve the data management problem science currently has.

### Reproducibility over publishability

Science arguably can't be science if it's not reproducible. However, there are many practical hurdles for it to be so. First, the "paper" format, even if it's paper-with-embedded-links, is not reproducible *per se* (even if tools such as the one proposed by Kardas et al. [@kardas2020axcell] are going to be able to extract at least the *results* in papers, not the code and experimental setup). Efforts like *Papers with Code* [@pwc] help associate something that has been already published with its corresponding code, but even if this is a step forward reproducibility, a effort similar to the one invested in deploying applications to production should be made: configuration as code, provision of all needed services, inclusion of all data inflows, as well as extensive testing. Testing that is an essential part of the agile mindset, and is severely lacking in scientific workflows.

<!-- from this follows deploy *and* publish -->

### Testing at all levels over proved hypotheses

Essentially, all papers try to prove something, to answer a research question or to prove a hypothesis over data that is characterized in a certain way. Datasets are static, and hypotheses are proved over provided datasets, which are increasingly available, although that is not necessarily a given.

Science practitioners know that working with raw data is always hard, and needs a series of steps to be suitable for use. Extracting the data is hard, checking that everything is correct is hard. Small changes could lead to invalidation of results.

This is why testing is essential. "Code that's not tested is broken", we could say dataflows and workflows that are not tested are broken. Besides basic testing (testing for duplication, invalid data, things like that), we should formulate tests on data to check that it keeps being in the same format, range and general characteristics that allow our hypothesis to be valid. So agile science would need to test data to start with, before using it as input for workflows, but it should also unit-test all software used, perform integration tests on data + software, and eventually transform the hypothesis into actual software tests that would *continuously* check if the hypothesis still holds.

Increasingly, and when the Internet itself, as well as myriad sensors and devices, is a continuous source of data, publishing a paper drawing conclusions over a small piece of data is valuable and helpful. Creating a tested, continuously deployed workflow that over and over again tests that hypothesis, and that has been released as free software to be integrated as input or middleware in other workflows is immensely more valuable. But needs another hypothesis

### Open over closed

Science should be reproducible, and this implies that software used or produced for it should be open too. However, it makes sense to emphasize the pliability and flexibility of free-software-as-science. If you want to configure increasingly complicated workflows that are going to be deployed continuously, a single non-open component would break them and make them impossible.

I would like to think this is the least controversial hypothesis that would support agile science. After all, it's been repeatedly proved [@vandewalle2012code][@vandewalle2019code] that papers with code have a higher impact than those who hide it or simply don't publish it alongside the paper itself. As a matter of fact, it's quite usual right now in many scientific conferences to accompany the paper with pointers to a GitHub repo; searching over GitHub for similar code might also help scientists/coders as much as reading new papers. Some important conferences like NeurIPS now have reproducibility responsibles in their program committee, and they have shown that 3 out of 4 papers in that conference already post their GitHub repository [@gibney2020ai]. This is mainly prevalent, however, in the data science/machine learning community [@wattanakriengkrai2020github].

There's a more important aspect to this: science in open repositories and with free licenses leaves research open to all stakeholders, who can have a say in its outcomes, as well as in their direction. Which is why we prefer:

### Stakeholder collaboration over vertical chains-of-command

Again, it has been the Covid crisis the occasion where the world has realized that it needed science, it needed a lot and it needed it now. The whole world health and even lives were at stake, and government officials as well as the civil society needed to know from what to do to avoid contagion to the evolution of the pandemic and what it would mean  vis-Ã -vis their return to a less restrictive situation.

In this, many open data repositories emerged with partial, localized, solutions, that allowed people in general and maybe also government officials have a bit of more control over their lives.

As a matter of fact, the scientific chain on command and general career environment causes not a few problems in participants [@Levecque2017WorkOA] . This study mentions explicitly "the supervisor's leadership style" as one of the factors that are linked to health problems, with the general work and organizational context cited as one of the highlights of the study. The scenario shown in a recent Nature survey [@woolston2019phds] would be unsustainable in any kind of company, be it software development or any other kind of company. Agile software development offers a series of principles and best practices that enable a sustainable pace of development, with regular deployments; but since it values "individuals and interactions over processes and tools" it also creates an humane, self-organizing work environment that results in better worker satisfaction, as well as sustainable, career-long learning. That should also be an objective in scientific research.

Techniques such as Scrum [@9140255] have helped in data science (and this, only recently); again, this has not extended to computer science or, in general, science at large.

## The way forward

Agile fixed software development by proposing a series of principles attached to the Agile manifesto [@beck2001manifesto] that eventually spawned a series of tools on one hand, and best practices in other hand. Tools that can be grouped into generic CI and CD toolchains, including MLOps tools [@makinen2021needs] , team work tools (usually attached to source repositories) such as Jira or GitHub itself and the use of different methodologies (Kanban, Scrum), practices (reviews, retros) and roles (product owner, stakeholder)
to streamline software production, bring value to stakeholders, and provide a sane, stable nurturing and eventually productive working environment.

Science should not be different, and a (roughly) direct translation of all these practices, however they are interpreted, would be beneficial. We'll try, anyway, to delve a bit further into those concepts to see how they translate and how they could be applied, in practice, to science.

### The product needs to be a deployed workflow

We need to shift focus from publishing on paper as a one shot to deploying working workflows, which will have as side products papers that can be continuously updated or else simply remain as a snapshot of the state of the art in a particular point in time, but that can, anyway, be re-produced (see one of the hypothesis above, reproducibility above all else) by anyone, but specially the people that have produced it first.

As we see this shift in action, we will see how the science system changes to accommodate this and take it into account in a scientific career. At this point in time, the peer-review and article-publishing system has been gamed in so many ways \cite{arneyBroken}, that it's almost meaningless to rely on it for scientific promotion, not to mention science itself. To a certain point, it's become an obstacle to science. But replicability can fix it [@moonesinghe2007most], and the best way to totally replicate results is to publish them openly (third hypothesis above).

### The product owner is the team producign it



## Conclusions


## References